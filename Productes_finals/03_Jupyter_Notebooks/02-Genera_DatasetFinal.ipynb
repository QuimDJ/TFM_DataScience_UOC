{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "    <p style=\"margin: 0; padding-top: 22px; text-align:right;\"><b>Treball de Final de Màster </b></p>\n",
    "<p style=\"margin: 0; text-align:right;\"><b>ANÀLISI DEL DIA MUNDIAL DE LES MALALTIES MINORITÀRIES (Font de dades: TWITTER)</b>.</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "\n",
    "# PAC 3: Disseny i Implementació del TFM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERACIÓ DEL DATASET DE DADES\n",
    "<p style=\"color:#0000FF\">\n",
    "En aquest Jupyter Notebook, veurem com generar el dataset de dades final, a partir de dades prèviament captades a Twitter i emmagatzemades a\n",
    "una base de dades documental MongoDB.\n",
    "El procés de generació del dataset i preprocessament dels tuits captats, implica la gestió de les traduccions del seu idioma d'origen a l'anglès i la neteja\n",
    "del text, detecció d'emojis i la seva normalització, així com l'obtenció de camps d'interès per la modelització, organitzant-los de manera tabular i \n",
    "un generant una còpia a disc en format Excel.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:right\">\n",
    "    <span style=\"font-family:calibri;font-size:8;color:white; background-color:Crimson; text-align:right; margin-left: 0.5cm;padding:0.3cm\"> Autor: </span>\n",
    "    <span style=\"font-family:calibri;font-size:8;color:white; background-color:RoyalBlue; text-align:center;  margin-right: 0.5cm; padding:0.3cm\"><b> Joaquim de Dalmases Juanet </b></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:#0000FF\">\n",
    "    Partim de la suposició de l'existència d'una base de dades MongoDB on ja tenim totes les dades a modelar.\n",
    "</p>\n",
    "<p style=\"color:#0000FF\">\n",
    "    El procés que cal implementar consisteix a connectar a la base de dades i mitjançant una consulta i generar un format de dataset per modelar.\n",
    "    En el cas dels tuits en anglès, ja tindríem el dataset, si no, cal diferenciar entre els tuits en castellà i la resta d'idiomes.\n",
    "    El procés de traducció s'ha separat de la generació del dataset, ja que es pot fer de manera independent i cadascú escollir el mètode que consideri més adient.\n",
    "    Un primer bloc està constituït pels tuits d'un idioma diferent de l'anglès i el castellà. Els tuits en castellà pel seu volum considerable, han necessitat un segon bloc, i finalment els tuits en anglès formen part d'un tercer bloc de gestió. L'objectiu ha estat generar un corpus de text en anglès per modelitzar. Part d'aquesta etapa de preprocessament de les dades comprèn el càlcul de la polaritat ia subjectivitat del text, els emojis continguts a cada tuit, així com l'extracció d'elements de context que ens permetran enriquir el procés d'anàlisi del text com hashtags, mencions i dades quantitatives com el nombre de seguidors ('followers'), amics ('friens') de l'emisor del tuit.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llibreries utilitzades\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer\n",
    "from nltk.stem import LancasterStemmer, PorterStemmer\n",
    "from datetime import datetime\n",
    "from datetime import timedelta  \n",
    "import string\n",
    "import re, collections\n",
    "import time\n",
    "from textblob import TextBlob\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En aquest apartat definim els paràmetres globals per tal de llegir la base de dades \n",
    "# i generar el dataset de tuits que utilitzarem en la pipeline del processament de tuits.\n",
    "\n",
    "# Connexió a la base de dades MongoDB (usem les llibreria pymongo per python).\n",
    "\n",
    "# Utilitzem un servidor local i el port estandard 27017.\n",
    "client = MongoClient('mongodb://localhost:27017/?readPreference=primary&appname=py&ssl=false')\n",
    "\n",
    "# Base de dades: El nom de la base de dades és 'DM_MM2020' (Dia Mundial Malalties Minoritaries 2020)\n",
    "bbdd=\"DM_MM2020\"\n",
    "# La col·leció on guardem els documents és: 'Twitter'.\n",
    "coleccio=\"Twitter\"\n",
    "\n",
    "# Cal conservar format.\n",
    "DATA_INICI_PERIODE = \"13/02/2020\"\n",
    "DATA_FINAL_PEIODE = \"30/03/2020\"\n",
    "\n",
    "\n",
    "# Si el servidor de dades no fos local podríem tenir les següents opcions:\n",
    "# A) Una opció professional basada en cloud o núvol amb ossibilitat de disposar un clúster:\n",
    "#    Ex:  mongodb+srv://m220project:m220password@mflix-rsmx1.mongodb.net/test?authSource=admin&\n",
    "#                       replicaSet=mflix-shard-0&readPreference=primary&appname=MongoDB%20Compass&ssl=true\n",
    "#\n",
    "# B) Utilitzar una màquina virtual habilitada per algun port específic:\n",
    "#    Ex: mongodb://localhost:7017/?replicaSet=&readPreference=primary&appname=MongoDB%20Compass&ssl=false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_EMOJI = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # simbols i pictogrames\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # simbols de transport i senyalització/icones\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # caràcters xinesos\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "        u\"\\uf7e2\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "# Extreiem els emojis/emoticons del text del tuit.\n",
    "def cerca_emojis(text):\n",
    "    return RE_EMOJI.findall(text)\n",
    "\n",
    "# Extreiem la data del tuit.\n",
    "def obte_dataHora(tuit):\n",
    "    return datetime.strptime(tuit['created_at'], \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "\n",
    "# Extreiem els hashtags.\n",
    "def obte_hashtags(tuit):\n",
    "    return [marca['text'] for marca in tuit['entities']['hashtags']]\n",
    "\n",
    "#Extreu mencions (referències a usuari).\n",
    "def obte_mencions(tweet):\n",
    "    return [m['screen_name'] for m in tweet['entities']['user_mentions']]\n",
    "\n",
    "def NetejaNorm(text_o,idioma='english', stem=0, meta=0):\n",
    "    text=\"\"\n",
    "    text_normalitzat =\"\"\n",
    "    # Calculem tokens\n",
    "    tweet_tokenizer = TweetTokenizer()\n",
    "    tokens = tweet_tokenizer.tokenize(text_o)\n",
    "    # Crea un element anomenat 'stemmer' que selecciona l'arrel comuna de la paraula.\n",
    "    # evitant al text diverses formes d'una mateixa paraula. \n",
    "    # Ex: cantar és 'stem' o troncal de cantat, cantada, cantaria\n",
    "    if stem==0:\n",
    "        stemmer = LancasterStemmer()\n",
    "    else:\n",
    "        stemmer = PorterStemmer() # Opció menys agressiva i més antiga.\n",
    "    # Processa cada token del text i aplica totes les regles de netejat\n",
    "    # i finalment aplica steamming per obtenir el text normalitzat.\n",
    "    for elem in tokens:\n",
    "        # Estandaritzem el tex a minúscules.\n",
    "        elem=elem.lower()\n",
    "        # Processem sempre i quan no sigui una 'stopword'.\n",
    "        if elem  not in stopwords.words(idioma):\n",
    "            # Elimina URL's\n",
    "            text_net=re.sub(r\"\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*\", '',elem,flags=re.UNICODE)\n",
    "            text_net=text_net.replace(\"https\",\"\")\n",
    "            text_net=text_net.replace(\"http\",\"\")\n",
    "            if len(text_net)>1:\n",
    "                # Eliminem la informació de context:\n",
    "                if meta==0:\n",
    "                    # Elimina hashtags\n",
    "                    text_net=re.sub(r'#([^\\s]+)', \"\", text_net)\n",
    "                    # Elimina referencies a usuari\n",
    "                    text_net=re.sub(r'@([^\\s]+)', \"\", text_net)\n",
    "                if len(text_net)>1:\n",
    "                    # Obté text alfanumeric (substitueix tot caràcter que no és alfanumèric per \"\")\n",
    "                    text_net = re.sub('([^\\w\\'+]|[0-9]+)',' ',text_net).strip()\n",
    "                    if len(text_net)>1:\n",
    "                        # Neteja caracteres en blanc, marques 'rt'\n",
    "                        text_net = re.sub('[\\t\\n\\r\\f\\v]+','', text_net)\n",
    "                        if len(text_net)>1:\n",
    "                            text_net = re.sub(r' +',' ', text_net,flags=re.UNICODE)\n",
    "                            # cadenes repetitives de caràcters\n",
    "                            text_net=re.sub(r\"([A-Za-z])\\1+\", r'\\1\\1',text_net,flags=re.UNICODE)\n",
    "                            if len(text_net)>1:\n",
    "                                if text_net.startswith(\"rt\"):\n",
    "                                    text_net = text_net.replace('rt','',1)\n",
    "                                if len(text_net)>1:\n",
    "                                    # En paraules apostrofades ens quedem amb la part no apostrofada.\n",
    "                                    text_net = text_net.split(\"'\")[0]\n",
    "                                    # Filtrem les paraules d'un sol caràcter residuals.\n",
    "                                    # Reconstruim usant dues llistes, el text del tuit netejat\n",
    "                                    # i el text normalitzat.\n",
    "                                    if len(text_net)>1:\n",
    "                                        if text==\"\":\n",
    "                                            text = text_net\n",
    "                                            text_normalitzat = stemmer.stem(text_net)\n",
    "                                        else:\n",
    "                                            text = text + \" \" + text_net\n",
    "                                            # Cerquem l'arrel o paurala base en cada mot del text detectat.\n",
    "                                            text_normalitzat = text_normalitzat + \" \" + stemmer.stem(text_net)      \n",
    "    # Si el tuit en el procés de neteja queda buit \n",
    "    # el representem per un caràcter en blanc per compatibilitat al exportar a fitxer excel.\n",
    "    if len(text)==0: \n",
    "        text = \" \"\n",
    "        text_normalitzat = \" \"\n",
    "    ftext=text+'@'+text_normalitzat\n",
    "    return ftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generaDataset_origenDM_MM2020(bd, colec, cli, idiomes, dataI=\"13/02/2020\", dataF=\"30/03/2020\"):\n",
    "    # Paràmetres d'entrada: Data inicial i data final del periode de temps\n",
    "    # per el qual volem  extreure tots els tuits de la base de dades.\n",
    "\n",
    "    # Data Inicial.\n",
    "    DataI=dataI\n",
    "    # Data Final.\n",
    "    DataF=dataF\n",
    "\n",
    "    # Configuracions dels codis de formats de data i hora per dates.\n",
    "    #\"%a (nom dia semana, tres lletres) %b (nom mes 3 lletres) \n",
    "    # %d (dia en digits)    %H:%M:%S (Hores, minuts, segons) +0000 %Y (any 4 digits)\"\n",
    "\n",
    "    # Conversions a format data, de la data inicial i final.\n",
    "    di=datetime.strptime(DataI, \"%d/%m/%Y\")\n",
    "    df=datetime.strptime(DataF, \"%d/%m/%Y\")\n",
    "    # Calcula el nombre de dies comprés en el periode de dates indicat a 'DataI' i 'DataF'\n",
    "    periode=(df-di)\n",
    "    # 'time_start0' instant de temps inicial per comptabilitzar tot el temps\n",
    "    # total de processament del periode complet.\n",
    "    time_start0 = time.time()\n",
    "    # La variable 'primer' controla que en un dia existeixin tuits corresponents a la consulta que realitzarem a la base de dades.\n",
    "    primer=True\n",
    "    # Registra el temps total de processament.\n",
    "    total=0\n",
    "    if idiomes=={\"lang\":{\"$not\":{\"$in\":[\"es\",\"en\",\"und\"]}}}: idioma=\"Altres\"\n",
    "    elif idiomes=={\"lang\":{\"$in\":[\"es\"]}}: idioma=\"es\"\n",
    "    else: idioma=\"en\"\n",
    "    # La llibreria NLTK, necessita configurar les stop words, (per defecte suposarem anglès)\n",
    "    if idioma==\"es\":\n",
    "        stopWordsIdioma=\"spanish\"\n",
    "    else:\n",
    "        stopWordsIdioma=\"english\"\n",
    "    # La variable t (unitat:dies) identifica numericament a cada dia del periode de dies definit.\n",
    "    # Ens a serveix per calcular cada data en referència a la data inicial.\n",
    "    for t in range(periode.days+1):\n",
    "        # 'time_start' instant de temps d'inici per cada dia. Ens permetrà calcular \n",
    "        # el temps empreat per processar tots els tuits de cada dia.\n",
    "        time_start = time.time()\n",
    "        # 'dt' conté la data actual en procés.\n",
    "        dt=di + timedelta(days=t)\n",
    "        # 'data' valor de data per realitzar la consulta i obtenir tots els tuits d'un dia en la variable 'filter'\n",
    "        data=dt.strftime(\"%a %b %d\")\n",
    "        # 'label' Etiqueta text per imprimir en la sortida per pantalla de línia d'informació dinàmica\n",
    "        # que es mostra quan s'executa el codi.\n",
    "        label=dt.strftime(\"%a %b %d %Y\")\n",
    "        print(label)\n",
    "        # Inicialització de la variable 'm' que registra el nombre de tuits per cada dia.\n",
    "        m=0\n",
    "        # Cal definir els paràmetres 'filter' i 'project' per implementar la consulta \n",
    "        # amb el mètode Find. \n",
    "        # En aquest cas només usarem \"filter\":\n",
    "        # Ex:\n",
    "        #     filter={\"created_at\":{\"$regex\":data}, \"lang\":idioma} #, \"lang\":idioma\n",
    "\n",
    "        # En aquest bucle, anem obtenint en cada pas els tuits d'un dia del periode definit que no \n",
    "        # estan escrits en idioma anglès ni castellà o no tenen un alor indefinit en el camp 'lang'.\n",
    "        # El camp 'lang' en el 'Tweet object' de l'API de Twitter representa l'idioma.\n",
    "        # Ex: filter={\"$and\":[{\"created_at\":{\"$regex\":data}},{\"lang\":{\"$not\":{\"$in\":[\"es\",\"en\",\"und\"]}}}]}\n",
    "\n",
    "        filter={\"$and\":[{\"created_at\":{\"$regex\":data}},idiomes]}\n",
    "        # Executem la consulta a la base de dades MongoDB.\n",
    "        # Obtenim un cursor amb la llista de documents coincidents del dia en procés i l'idioma indicat.\n",
    "        result=cli[bd][colec].find(filter=filter)\n",
    "\n",
    "        # Generem un pandas DataFrame (llibreria 'pandas' ens permet definir dades tabulars) amb la resposta obtinguda.\n",
    "        df = pd.DataFrame.from_records(result)\n",
    "        # En cas d'obtenir un nombre de tuits major a zero (files del DataFrame > 0), comencem el seu processament:\n",
    "        if df.shape[0]>0:\n",
    "            # Llista on guardarem tots els textos de tuit netejats \n",
    "            # és el que anomenem 'Tokenització'. El procés de tokenització obté unitats d'informació significativa del text.\n",
    "            textClean=[]\n",
    "            # Llista amb els textos originals: 2 cassos: extended tweet/ normal tweet\n",
    "            text_original = []\n",
    "            # Llista amb els textos de tuits normalitzats.\n",
    "            textNorm=[]\n",
    "            # Fraccionem les dates, a 'diaSem' guardem el nom del dia de la semana.\n",
    "            diaSem=[]\n",
    "            # A 'dia','mes' i 'yy' guardem: el digit del dia, el digit del mes i els 4 digits del any. \n",
    "            dia=[]\n",
    "            mes=[]\n",
    "            yy=[]\n",
    "            # A 'hora', 'minuts', 'segons' guardem el digit de l'hora en format 24h, \n",
    "            # els digits dels minuts i els dels segons.\n",
    "            hora=[]\n",
    "            minuts=[]\n",
    "            segs=[]\n",
    "            # A 'hashtags' tots els hashtags inclossos en cada tuit.\n",
    "            hashtags=[]\n",
    "            # A 'mentions' totes les mencions a usuari inclossos en cada tuit: format '@identificador_text'.\n",
    "            user_mentions=[]\n",
    "            # El nom d'usuari (username, camp 'name' al 'tweet object'), tot i que no l'utilitzarem per privacitat de dades.\n",
    "            user_names=[]\n",
    "            # El id text, ('string') a mode d'identificador de l'autor del tuit. El camp screen_name no l'hem registrat.\n",
    "            user_idstr=[]\n",
    "            # Llista on anirem guardant el nombre d'amics de l'autor (usuari) del tuit.\n",
    "            user_friends_c=[]\n",
    "            # Llista on anirem guardant el nombre de seguidors ('followers') de l'autor (usuari) del tuit.\n",
    "            user_followers_c=[]\n",
    "            # usuaris en llista de l'autor de cada tuit.\n",
    "            user_listed_c=[]\n",
    "            # La polaritat i la subjectivitat es deixen definides per quan puguin ser calculades.\n",
    "            # polaritat calculada segons el text netejat de cad tuit (usant la llibreria 'textlob').\n",
    "            polarity=[]\n",
    "            # Subjectivitat calculada segons el text de cada tuit (usant la llibreria 'textlob').\n",
    "            subjectivity=[]\n",
    "            # Llista de mojis\n",
    "            emojis_list = []\n",
    "\n",
    "            result.rewind()\n",
    "            # Bloc de codi per processar el tuit i omplir totes les llistes amb les dades\n",
    "            # que després s'usaran per construir el dataset que es vol generar per modelar.\n",
    "            # La variable 'm' controla quants tuits processem.\n",
    "            m=0\n",
    "            # La variable 'k' controla cada 'Tweet Object' contingut al cusor 'result',  resultat de la consulta.\n",
    "            for k in result:\n",
    "                m = m + 1\n",
    "                # Aplicar el 'Cleaning' o netejat del text del tuit. Aquest codi es versàtil per els tres cassos definits\n",
    "                # segons el valor de la variable 'idioma' i el valor de la variable 'filter' en la consulta\n",
    "                # els cassos: tuits en anglès, castellà i en 'altres' idiomes (cas que ens ocupa).             \n",
    "                # text original del tuit\n",
    "                if k['truncated']:\n",
    "                    text_o = k['extended_tweet']['full_text']\n",
    "                else:\n",
    "                    text_o = k['text']\n",
    "                if text_o.startswith(\"https://\"):\n",
    "                    text_o = \" \" + text_o\n",
    "                text_original.append(text_o)            \n",
    "                # Data text del tuit en tipus 'Date', util per extreure les dades posteriors.\n",
    "                data_t = obte_dataHora(k)\n",
    "                diaSem.append(data_t.strftime(\"%A\"))\n",
    "                dia.append(int(data_t.strftime(\"%d\")))\n",
    "                mes.append(int(data_t.strftime(\"%m\")))\n",
    "                yy.append(int(data_t.strftime(\"%Y\")))\n",
    "                hora.append(int(data_t.strftime(\"%H\")))\n",
    "                minuts.append(int(data_t.strftime(\"%M\")))\n",
    "                segs.append(int(data_t.strftime(\"%S\")))\n",
    "                # Extracció de tota la informació de context de cada tuit.\n",
    "                hashtags.append(obte_hashtags(k))\n",
    "                user_mentions.append(obte_mencions(k))\n",
    "                user_names.append(k['user']['name']) \n",
    "                user_idstr.append(k['user']['id_str'])\n",
    "                user_friends_c.append(k['user']['friends_count'])\n",
    "                user_followers_c.append(k['user']['followers_count'])\n",
    "                user_listed_c.append(k['user']['listed_count'])\n",
    "\n",
    "            # Si tenim tuits processats, m serà major que 0.     \n",
    "            if m>0:\n",
    "                # Generem un dataframe imatge del dataset que volem generar amb tota la informació recopilada.\n",
    "                df1 = pd.DataFrame({\"_id\":df._id,\"created_at\":df.created_at, \"text\":text_original,\"text_net\":None,'text_Norm':None, \\\n",
    "                                    'diaSem':diaSem,'dia':dia,'mes':mes,'yy':yy,'hora':hora,'minuts':minuts,'segs':segs, \\\n",
    "                                    'hashtags':hashtags,'user_mentions':user_mentions,'user_name':user_names,'user_idstr':user_idstr, \\\n",
    "                                    'user_friends_c':user_friends_c,'user_followers_c':user_followers_c,'user_listed_c':user_listed_c, \\\n",
    "                                    'retweet_count':df.retweet_count,'lang':df.lang, 'polarity': None, 'subjectivity': None, \\\n",
    "                                    'emojis':None})\n",
    "            # primer=True significa que és el primer dia amb tuits i que disposem d'un dataframe (df1) \n",
    "            # amb dades per acumular al dataframe final (df2) que està buit.\n",
    "            if primer and m>0:\n",
    "                df2=df1\n",
    "                primer=False\n",
    "            # disposem de tuits acumulats i cal afegir els del dia en procés.\n",
    "            elif primer==False and m>0:\n",
    "                frames=[df2,df1]\n",
    "                # Afegim les files del dataframe 'df1' al 'df2'\n",
    "                df2=pd.concat(frames)\n",
    "            # Encara no disposem de tuits.\n",
    "            else:\n",
    "                print(\"No existeixen tuits aquest periode: {}.\".format(label))\n",
    "        # 'temps' registra el temps parcial diari de processament de tuits.\n",
    "        temps=(time.time()-time_start)/60\n",
    "        print(\"#Tuits processats:\",m,\"\\n Durada: \",int(temps) if temps>0 else 0,\"minut/s \", \\\n",
    "              int((temps-int(temps))*60),\"segons.\")\n",
    "        t=t+1\n",
    "    # Exportem tot el processament realitzat i guardat a 'df2' a un fitxer de notació:\n",
    "    # DM_MM2020_Twitter_13-02-2020_30-03-2020_altres.xlsx o bé informem de que no se'n disposa.\n",
    "    if primer==False:\n",
    "        fitxerExportacio = '{}_{}_{}_{}.xlsx'.format(bbdd,coleccio,(DataI+\"_\"+DataF).replace(\"/\",\"-\"),idioma)\n",
    "        # Generem un index per identificar els tuits.\n",
    "        df2=df2.reset_index(drop=True)\n",
    "        df2['n']=df2.index+1\n",
    "        df2.to_excel(fitxerExportacio,index=0)\n",
    "        total=(time.time()-time_start0)/60\n",
    "        print(\"Exportat al fitxer\",fitxerExportacio, \\\n",
    "              \"\\nProcessament finalitzat!\\n Durada Final: \",int(total) if total>0 else 0,\"minut/s \", \\\n",
    "              int((total-int(total))*60),\"segons.\")\n",
    "    else:\n",
    "        print(\"No existeixen tuits aquest periode: {}.\".format(DataI + \" a \" + DataF))\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 13 2020\n",
      "#Tuits processats: 2 \n",
      " Durada:  0 minut/s  1 segons.\n",
      "Fri Feb 14 2020\n",
      "#Tuits processats: 1 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sat Feb 15 2020\n",
      "#Tuits processats: 0 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sun Feb 16 2020\n",
      "#Tuits processats: 0 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Mon Feb 17 2020\n",
      "#Tuits processats: 1 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Tue Feb 18 2020\n",
      "#Tuits processats: 0 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Wed Feb 19 2020\n",
      "#Tuits processats: 79 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Thu Feb 20 2020\n",
      "#Tuits processats: 160 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Fri Feb 21 2020\n",
      "#Tuits processats: 141 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sat Feb 22 2020\n",
      "#Tuits processats: 82 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sun Feb 23 2020\n",
      "#Tuits processats: 70 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Mon Feb 24 2020\n",
      "#Tuits processats: 189 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Tue Feb 25 2020\n",
      "#Tuits processats: 403 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Wed Feb 26 2020\n",
      "#Tuits processats: 363 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Thu Feb 27 2020\n",
      "#Tuits processats: 464 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Fri Feb 28 2020\n",
      "#Tuits processats: 1168 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sat Feb 29 2020\n",
      "#Tuits processats: 4444 \n",
      " Durada:  0 minut/s  2 segons.\n",
      "Sun Mar 01 2020\n",
      "#Tuits processats: 895 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Mon Mar 02 2020\n",
      "#Tuits processats: 312 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Tue Mar 03 2020\n",
      "#Tuits processats: 138 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Wed Mar 04 2020\n",
      "#Tuits processats: 93 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Thu Mar 05 2020\n",
      "#Tuits processats: 39 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Fri Mar 06 2020\n",
      "#Tuits processats: 36 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sat Mar 07 2020\n",
      "#Tuits processats: 16 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sun Mar 08 2020\n",
      "#Tuits processats: 12 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Mon Mar 09 2020\n",
      "#Tuits processats: 15 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Tue Mar 10 2020\n",
      "#Tuits processats: 2 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Wed Mar 11 2020\n",
      "#Tuits processats: 4 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Thu Mar 12 2020\n",
      "#Tuits processats: 5 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Fri Mar 13 2020\n",
      "#Tuits processats: 2 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sat Mar 14 2020\n",
      "#Tuits processats: 2 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sun Mar 15 2020\n",
      "#Tuits processats: 14 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Mon Mar 16 2020\n",
      "#Tuits processats: 9 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Tue Mar 17 2020\n",
      "#Tuits processats: 2 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Wed Mar 18 2020\n",
      "#Tuits processats: 3 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Thu Mar 19 2020\n",
      "#Tuits processats: 2 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Fri Mar 20 2020\n",
      "#Tuits processats: 6 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sat Mar 21 2020\n",
      "#Tuits processats: 5 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sun Mar 22 2020\n",
      "#Tuits processats: 3 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Mon Mar 23 2020\n",
      "#Tuits processats: 0 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Tue Mar 24 2020\n",
      "#Tuits processats: 1 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Wed Mar 25 2020\n",
      "#Tuits processats: 2 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Thu Mar 26 2020\n",
      "#Tuits processats: 0 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Fri Mar 27 2020\n",
      "#Tuits processats: 1 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sat Mar 28 2020\n",
      "#Tuits processats: 3 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sun Mar 29 2020\n",
      "#Tuits processats: 4 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Mon Mar 30 2020\n",
      "#Tuits processats: 2 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Exportat al fitxer DM_MM2020_Twitter_13-02-2020_30-03-2020_Altres.xlsx \n",
      "Processament finalitzat!\n",
      " Durada Final:  0 minut/s  15 segons.\n"
     ]
    }
   ],
   "source": [
    "# Generem el dataset del periode complet dels tuits amb idioma diferent d'anglès, castellà i no definit:\n",
    "\n",
    "i={\"lang\":{\"$not\":{\"$in\":[\"es\",\"en\",\"und\"]}}}\n",
    "df2=generaDataset_origenDM_MM2020(bbdd, coleccio, client, idiomes=i, dataI=\"13/02/2020\", dataF=\"30/03/2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['_id', 'created_at', 'text', 'text_net', 'text_Norm', 'diaSem', 'dia',\n",
      "       'mes', 'yy', 'hora', 'minuts', 'segs', 'hashtags', 'user_mentions',\n",
      "       'user_name', 'user_idstr', 'user_friends_c', 'user_followers_c',\n",
      "       'user_listed_c', 'retweet_count', 'lang', 'polarity', 'subjectivity',\n",
      "       'emojis', 'n'],\n",
      "      dtype='object')\n",
      "(9195, 25)\n"
     ]
    }
   ],
   "source": [
    "# Mostrem l'estructura del dataframe calculat\n",
    "print(df2.columns)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = df2.filter([\"n\",\"text\",\"lang\"])\n",
    "# 'langs' conté tots els codis corresponents als idiomes diferents d'anglès i castellà \n",
    "#  dels tuits capturats en el periode d'estudi.\n",
    "codis = d2[\"lang\"]\n",
    "codis = codis.drop_duplicates()\n",
    "#df2.drop_duplicates(subset = [\"created_at\",\"user_idstr\", \"text\"], keep = False, inplace = True)\n",
    "codis.sort_values(inplace=True)\n",
    "langs=list(codis)\n",
    "#langs = [\"ar\",\"bg\",\"ca\",\"cs\",\"cy\",\"da\",\"de\",\"el\",\"et\",\"eu\",\"fa\",\"fi\",\"fr\",\"hi\",\"ht\",\"hu\", \\\n",
    "#       \"in\",\"it\",\"iw\",\"ja\",\"ko\",\"lt\",\"lv\",\"nl\",\"no\",\"pl\",\"pt\",\"ro\",\"ru\",\"sl\",\"sr\",\"sv\", \\\n",
    "#       \"te\",\"tl\",\"tr\",\"uk\",\"vi\",\"zh\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:#0000FF\">\n",
    "   Generem per cada idioma el fitxer excel amb els tuits en aquest idioma.<br>\n",
    "   Cal tenir cura de que el nom de les columnes sigui curt ja que les traduccions no els haurien de poder canviar, i així no tenir problemes al fusionar tots els arxius.<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(langs)):\n",
    "    dt = d2[d2.lang==langs[k]].copy()\n",
    "    dt.filter([\"n\",\"text\"]).to_excel(\"altres_\"+langs[k]+\".xlsx\",index=0)\n",
    "l=list(dt)\n",
    "l=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:#0000FF\">\n",
    "Els fitxers generats tenen la notació: \"altres_XX.xlsx\" on XX és el codi estàndar de l'idioma.\n",
    "El codi de llenguatge es pot formatar com a ISO 639-1 alpha-2 (en), ISO 639-3 alpha-3 (msa) o \n",
    "ISO 639-1 alpha-2 combinat amb una localització ISO 3166-1 alfa-2 (zh -tw).\n",
    "Aquests fitxers en el cas del bloc 'ALTRES' séràn traduits manualment perquè la detecció\n",
    "automàtica no sempre és acurada i ens assegurem una bona traducció.\n",
    "</p>\n",
    "<p style=\"color:#0000FF\">\n",
    "Per tant, ara suposem que ja tenim uns excels amb notació: \"altres_XX_traduit.xlsx\" i procedim a fusionar-los generant una\n",
    "    exportació intermitja al fitxer excel amb notació \"Altres_traduits_Final.xlsx\", corresponent als idiomes diferents a l'anglès i castellà.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_ini emmagatzema la fusió de totes les traduccions.\n",
    "file_ini=pd.read_excel('Altres_' + langs[0] + '_traduit.xlsx')\n",
    "for k in range(1,len(langs)):\n",
    "    file_seg=pd.read_excel('Altres_' + langs[k] + '_traduit.xlsx')\n",
    "    if not np.all(file_seg.columns==[\"n\",\"text\"]):\n",
    "        print(langs[k])\n",
    "    file_ini=file_ini.append(file_seg,ignore_index=True)\n",
    "\n",
    "file_ini=file_ini.sort_values(['n'])\n",
    "file_ini.to_excel(\"Altres_traduits_Final.xlsx\",index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construim un join per columna 'n' per tal d'alinear en el dataset\n",
    "# les traducions de cada idioma amb els seus textos de tuit originals.\n",
    "dataset_altres_idiomes = pd.merge(df2, file_ini, on='n', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un cop fet el join eliminem la columna 'n' i obtenim el dataset d'aquest bloc de tuits.\n",
    "dataset_altres_idiomes=dataset_altres_idiomes.loc[:, dataset_altres_idiomes.columns != 'n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Tuits processats: 9195 \n",
      "#Tuits duplicats: 0 \n",
      "#Tuits finals: 9195 \n",
      " Durada:  0 minut/s  0 segons.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9195, 25)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminem duplicats\n",
    "num=df2.shape[0]\n",
    "time_start = time.time()\n",
    "df2.drop_duplicates(subset = [\"created_at\",\"user_idstr\", \"text\"], keep = 'first', inplace = True) \n",
    "temps=(time.time()-time_start)/60\n",
    "print(\"#Tuits processats:\",num, \"\\n#Tuits duplicats:\",num-df2.shape[0],\"\\n#Tuits finals:\",df2.shape[0], \\\n",
    "      \"\\n Durada: \",int(temps) if temps>0 else 0,\"minut/s \", \\\n",
    "      int((temps-int(temps))*60),\"segons.\")\n",
    "dataset_altres_idiomes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text_x</th>\n",
       "      <th>text_net</th>\n",
       "      <th>text_Norm</th>\n",
       "      <th>diaSem</th>\n",
       "      <th>dia</th>\n",
       "      <th>mes</th>\n",
       "      <th>yy</th>\n",
       "      <th>hora</th>\n",
       "      <th>...</th>\n",
       "      <th>user_idstr</th>\n",
       "      <th>user_friends_c</th>\n",
       "      <th>user_followers_c</th>\n",
       "      <th>user_listed_c</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>lang</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>emojis</th>\n",
       "      <th>text_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5e78e8104e54db2148d197d0</td>\n",
       "      <td>Thu Feb 13 12:46:37 +0000 2020</td>\n",
       "      <td>RT @diputadospan: Conferencia de prensa #Enfer...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>766788351928176640</td>\n",
       "      <td>477</td>\n",
       "      <td>219</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>pt</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>RT @diputadospan: Press conference #Enfermedad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id                      created_at  \\\n",
       "0  5e78e8104e54db2148d197d0  Thu Feb 13 12:46:37 +0000 2020   \n",
       "\n",
       "                                              text_x text_net text_Norm  \\\n",
       "0  RT @diputadospan: Conferencia de prensa #Enfer...     None      None   \n",
       "\n",
       "     diaSem  dia  mes    yy  hora  ...          user_idstr  user_friends_c  \\\n",
       "0  Thursday   13    2  2020    12  ...  766788351928176640             477   \n",
       "\n",
       "  user_followers_c user_listed_c retweet_count lang  polarity  subjectivity  \\\n",
       "0              219             2             0   pt      None          None   \n",
       "\n",
       "   emojis                                             text_y  \n",
       "0    None  RT @diputadospan: Press conference #Enfermedad...  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On per cada tuit, el camp \"text_x\" és el texte original i \"text_y\" és el trauït a l'anglès.\n",
    "dataset_altres_idiomes.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 13 2020\n",
      "#Tuits processats: 1217 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Fri Feb 14 2020\n",
      "#Tuits processats: 282 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sat Feb 15 2020\n",
      "#Tuits processats: 243 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sun Feb 16 2020\n",
      "#Tuits processats: 159 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Mon Feb 17 2020\n",
      "#Tuits processats: 218 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Tue Feb 18 2020\n",
      "#Tuits processats: 69 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Wed Feb 19 2020\n",
      "#Tuits processats: 281 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Thu Feb 20 2020\n",
      "#Tuits processats: 356 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Fri Feb 21 2020\n",
      "#Tuits processats: 658 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sat Feb 22 2020\n",
      "#Tuits processats: 324 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sun Feb 23 2020\n",
      "#Tuits processats: 350 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Mon Feb 24 2020\n",
      "#Tuits processats: 676 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Tue Feb 25 2020\n",
      "#Tuits processats: 625 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Wed Feb 26 2020\n",
      "#Tuits processats: 865 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Thu Feb 27 2020\n",
      "#Tuits processats: 1059 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Fri Feb 28 2020\n",
      "#Tuits processats: 4117 \n",
      " Durada:  0 minut/s  1 segons.\n",
      "Sat Feb 29 2020\n",
      "#Tuits processats: 14890 \n",
      " Durada:  0 minut/s  5 segons.\n",
      "Sun Mar 01 2020\n",
      "#Tuits processats: 4636 \n",
      " Durada:  0 minut/s  1 segons.\n",
      "Mon Mar 02 2020\n",
      "#Tuits processats: 1284 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Tue Mar 03 2020\n",
      "#Tuits processats: 574 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Wed Mar 04 2020\n",
      "#Tuits processats: 434 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Thu Mar 05 2020\n",
      "#Tuits processats: 1132 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Fri Mar 06 2020\n",
      "#Tuits processats: 468 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sat Mar 07 2020\n",
      "#Tuits processats: 290 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sun Mar 08 2020\n",
      "#Tuits processats: 151 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Mon Mar 09 2020\n",
      "#Tuits processats: 187 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Tue Mar 10 2020\n",
      "#Tuits processats: 178 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Wed Mar 11 2020\n",
      "#Tuits processats: 152 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Thu Mar 12 2020\n",
      "#Tuits processats: 306 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Fri Mar 13 2020\n",
      "#Tuits processats: 271 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sat Mar 14 2020\n",
      "#Tuits processats: 95 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sun Mar 15 2020\n",
      "#Tuits processats: 102 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Mon Mar 16 2020\n",
      "#Tuits processats: 111 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Tue Mar 17 2020\n",
      "#Tuits processats: 48 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Wed Mar 18 2020\n",
      "#Tuits processats: 67 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Thu Mar 19 2020\n",
      "#Tuits processats: 58 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Fri Mar 20 2020\n",
      "#Tuits processats: 58 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sat Mar 21 2020\n",
      "#Tuits processats: 56 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sun Mar 22 2020\n",
      "#Tuits processats: 57 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Mon Mar 23 2020\n",
      "#Tuits processats: 1 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Tue Mar 24 2020\n",
      "#Tuits processats: 68 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Wed Mar 25 2020\n",
      "#Tuits processats: 128 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Thu Mar 26 2020\n",
      "#Tuits processats: 82 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Fri Mar 27 2020\n",
      "#Tuits processats: 77 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sat Mar 28 2020\n",
      "#Tuits processats: 41 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sun Mar 29 2020\n",
      "#Tuits processats: 84 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Mon Mar 30 2020\n",
      "#Tuits processats: 91 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Exportat al fitxer DM_MM2020_Twitter_13-02-2020_30-03-2020_es.xlsx \n",
      "Processament finalitzat!\n",
      " Durada Final:  0 minut/s  41 segons.\n"
     ]
    }
   ],
   "source": [
    "# Calculem el dataset per els tuits en castellà\n",
    "# Processem el tuits per generar el dataset segons el periode complet:\n",
    "i={\"lang\":{\"$in\":[\"es\"]}}\n",
    "df_es = generaDataset_origenDM_MM2020(bbdd, coleccio, client, idiomes=i, dataI=\"13/02/2020\", dataF=\"30/03/2020\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:#0000FF\">\n",
    "En aquest punt del procés, hem guardat en format Excel tots els tuits en castellà, que cal traduir a l'anglès.<br>\n",
    "Es poden usar els marges gratuits dels traductors com: Google Translator, IBM Watson (IBM Cloud: servei Language Transaltion), Apertium, Yandex etc... o bé un servei de pagament.\n",
    "<br> A continuació, suposem que tenim un fitxer resultat de traduir el fitxer generat 'DM_MM2020_Twitter_13-02-2020_30-03-2020_es.xlsx', amb nom '<b>Es_traduits_Final.xlsx</b>'<br>\n",
    "En el nostre cas hem utilitzat IBM Watson i IBM Cloud, fent servir un servei de pagament anomenat '<b>Language Translator</b>' on pel primer 1.000.000 de mots la traducció és gratuita. <br>Generem l'arxiu del dataset final per aquests tuits. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definim la capçalera de manera que més tard, podem afegir els registres del bloc 'Altres'\n",
    "df_es = df_es.loc[:, df_es.columns != 'n']\n",
    "df_es.columns=['_id', 'created_at', 'text_x', 'text_net', 'text_Norm', 'diaSem', 'dia','mes', 'yy', 'hora', 'minuts', 'segs', \\\n",
    "             'hashtags', 'user_mentions', 'user_name', 'user_idstr', 'user_friends_c', 'user_followers_c', 'user_listed_c', \\\n",
    "             'retweet_count', 'lang', 'polarity', 'subjectivity','emojis']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37676, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Llegim el fitxer creat apart amb les traduccions.\n",
    "df_es_traduit = pd.read_excel(\"Es_traduits_Final.xlsx\")\n",
    "df_es_traduit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afegim la columna de les traduccions al camp 'text_y'\n",
    "df_es['text_y']=df_es_traduit['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Tuits processats: 37676 \n",
      "#Tuits duplicats: 5 \n",
      "#Tuits final: 37671 \n",
      " Durada:  0 minut/s  0 segons.\n"
     ]
    }
   ],
   "source": [
    "# Eliminem duplicats\n",
    "num=df_es.shape[0]\n",
    "time_start = time.time()\n",
    "df_es.drop_duplicates(subset = [\"created_at\",\"user_idstr\", \"text_x\"], keep = 'first', inplace = True) \n",
    "temps=(time.time()-time_start)/60\n",
    "print(\"#Tuits processats:\",num, \"\\n#Tuits duplicats:\",num-df_es.shape[0],\"\\n#Tuits final:\",df_es.shape[0],\"\\n Durada: \",int(temps) if temps>0 else 0,\"minut/s \", \\\n",
    "      int((temps-int(temps))*60),\"segons.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46866, 25)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fusionem els tuits en castellà traduits amb els traduits de diferents idiomes.\n",
    "df_es_altres = df_es.append(dataset_altres_idiomes,ignore_index=True)\n",
    "df_es_altres.reset_index(drop=True)\n",
    "df_es_altres.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:#0000FF\">\n",
    "    Finalment només ens queda afegir tots els tuits que estan en anglès i que no cal traduir.<br>\n",
    "    Un cop generat el dataset final, calcularem la polaritat i la subjectivitat sobre tot el dataset final.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 13 2020\n",
      "#Tuits processats: 4 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Fri Feb 14 2020\n",
      "#Tuits processats: 1 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sat Feb 15 2020\n",
      "#Tuits processats: 2 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sun Feb 16 2020\n",
      "#Tuits processats: 4 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Mon Feb 17 2020\n",
      "#Tuits processats: 1 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Tue Feb 18 2020\n",
      "#Tuits processats: 2 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Wed Feb 19 2020\n",
      "#Tuits processats: 473 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Thu Feb 20 2020\n",
      "#Tuits processats: 471 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Fri Feb 21 2020\n",
      "#Tuits processats: 672 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sat Feb 22 2020\n",
      "#Tuits processats: 539 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sun Feb 23 2020\n",
      "#Tuits processats: 374 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Mon Feb 24 2020\n",
      "#Tuits processats: 1356 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Tue Feb 25 2020\n",
      "#Tuits processats: 1574 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Wed Feb 26 2020\n",
      "#Tuits processats: 1587 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Thu Feb 27 2020\n",
      "#Tuits processats: 1902 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Fri Feb 28 2020\n",
      "#Tuits processats: 6071 \n",
      " Durada:  0 minut/s  2 segons.\n",
      "Sat Feb 29 2020\n",
      "#Tuits processats: 24420 \n",
      " Durada:  0 minut/s  9 segons.\n",
      "Sun Mar 01 2020\n",
      "#Tuits processats: 6733 \n",
      " Durada:  0 minut/s  3 segons.\n",
      "Mon Mar 02 2020\n",
      "#Tuits processats: 2168 \n",
      " Durada:  0 minut/s  1 segons.\n",
      "Tue Mar 03 2020\n",
      "#Tuits processats: 923 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Wed Mar 04 2020\n",
      "#Tuits processats: 735 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Thu Mar 05 2020\n",
      "#Tuits processats: 335 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Fri Mar 06 2020\n",
      "#Tuits processats: 269 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sat Mar 07 2020\n",
      "#Tuits processats: 150 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sun Mar 08 2020\n",
      "#Tuits processats: 92 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Mon Mar 09 2020\n",
      "#Tuits processats: 90 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Tue Mar 10 2020\n",
      "#Tuits processats: 73 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Wed Mar 11 2020\n",
      "#Tuits processats: 73 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Thu Mar 12 2020\n",
      "#Tuits processats: 75 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Fri Mar 13 2020\n",
      "#Tuits processats: 46 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sat Mar 14 2020\n",
      "#Tuits processats: 24 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sun Mar 15 2020\n",
      "#Tuits processats: 24 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Mon Mar 16 2020\n",
      "#Tuits processats: 50 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Tue Mar 17 2020\n",
      "#Tuits processats: 32 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Wed Mar 18 2020\n",
      "#Tuits processats: 23 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Thu Mar 19 2020\n",
      "#Tuits processats: 18 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Fri Mar 20 2020\n",
      "#Tuits processats: 20 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sat Mar 21 2020\n",
      "#Tuits processats: 18 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sun Mar 22 2020\n",
      "#Tuits processats: 16 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Mon Mar 23 2020\n",
      "#Tuits processats: 1 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Tue Mar 24 2020\n",
      "#Tuits processats: 17 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Wed Mar 25 2020\n",
      "#Tuits processats: 14 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Thu Mar 26 2020\n",
      "#Tuits processats: 24 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Fri Mar 27 2020\n",
      "#Tuits processats: 21 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sat Mar 28 2020\n",
      "#Tuits processats: 24 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Sun Mar 29 2020\n",
      "#Tuits processats: 6 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Mon Mar 30 2020\n",
      "#Tuits processats: 24 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Exportat al fitxer DM_MM2020_Twitter_13-02-2020_30-03-2020_en.xlsx \n",
      "Processament finalitzat!\n",
      " Durada Final:  0 minut/s  55 segons.\n"
     ]
    }
   ],
   "source": [
    "# Obtenim els tuits originalment en anglès \n",
    "i={\"lang\":{\"$in\":[\"en\"]}}\n",
    "df_en = generaDataset_origenDM_MM2020(bbdd, coleccio, client, idiomes=i, dataI=\"13/02/2020\", dataF=\"30/03/2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Tuits processats: 51571 \n",
      "#Tuits duplicats: 4 \n",
      "#Tuits final: 51567 \n",
      " Durada:  0 minut/s  0 segons.\n"
     ]
    }
   ],
   "source": [
    "# Eliminem duplicats\n",
    "num=df_en.shape[0]\n",
    "time_start = time.time()\n",
    "df_en.drop_duplicates(subset = [\"created_at\",\"user_idstr\", \"text\"], keep = 'first', inplace = True) \n",
    "temps=(time.time()-time_start)/60\n",
    "print(\"#Tuits processats:\",num, \"\\n#Tuits duplicats:\",num-df_en.shape[0],\"\\n#Tuits final:\",df_en.shape[0],\"\\n Durada: \",int(temps) if temps>0 else 0,\"minut/s \", \\\n",
    "      int((temps-int(temps))*60),\"segons.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurem les columnes amb el dataset actual i els afegim \n",
    "# per obtenir el dataset final.\n",
    "df_en = df_en.loc[:, df_en.columns != 'n']\n",
    "df_en.columns=['_id', 'created_at', 'text_x', 'text_net', 'text_Norm', 'diaSem', 'dia','mes', 'yy', 'hora', 'minuts', 'segs', \\\n",
    "             'hashtags', 'user_mentions', 'user_name', 'user_idstr', 'user_friends_c', 'user_followers_c', 'user_listed_c', \\\n",
    "             'retweet_count', 'lang', 'polarity', 'subjectivity','emojis']\n",
    "# Afegim la columna del text en anglès dels tuits el camp 'text_x', uniformitat i disposar en la columna text_y tots els tuits en anglès.\n",
    "df_en['text_y']=df_en['text_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98433, 25)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fusionem els tuits en casellà traduits i els traduits de diferents idiomes amb els originals en anglès.\n",
    "dataset_final = df_es_altres.append(df_en,ignore_index=True)\n",
    "dataset_final = dataset_final.sort_values(by=['_id'])\n",
    "dataset_final = dataset_final.reset_index(drop=True)\n",
    "dataset_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:#0000FF\">\n",
    "    Per implementar la fase de processament del dataset, calculem 5 camps:\n",
    "    <ol style=\"color:#0000FF\">\n",
    "        <li>El camp <b>text_net</b>, amb el text netejat.</li>\n",
    "        <li>El camp <b>text_Norm</b>, amb el text del tuit normalitzat: operació de 'Stemming'.</li>\n",
    "        <li>El camp <b>emojis</b> amb els emojis extrets del tuit.</li>\n",
    "        <li>Un camp amb la <b>polaritat</b> del text de cada tuit.<br>\n",
    "        La propietat 'sentiment' d'un objecte 'texblob', retorna una estructura de dades, <b>Sentiment (polaritat, subjectivitat)</b>. La puntuació de <b>polaritat</b> és un nombre decimal dins del rang [-1,0, 1.0]. Un valor -1.0 indica una polaritat negativa, un valor 0 neutral i un valor 1.0 una polaritat positiva o a favor del contingut del tuit. \n",
    "        </li>\n",
    "        <li>Un camp amb la <b>subjectivitat</b> del text cada tuit.<br>\n",
    "        La <b>subjectivitat</b> és un nombre decimal dins del rang [0.0, 1.0] on 0.0 és gens objectiu i 1.0 és totalment subjectiu.\n",
    "        </li>\n",
    "     </ol>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depuraTextTuit(dataset,idioma):\n",
    "# Funció que aplica sobre tots els tuits traduits, les regles de\n",
    "# netejat del text i normalització. A més a més per l'anàlisi de\n",
    "# sentiment, calcula la polaritat i la subjectivitat del text.\n",
    "\n",
    "    textClean=[]\n",
    "    textNorm=[]\n",
    "    emojis=[]\n",
    "    polarity = []\n",
    "    subjectivity = []\n",
    "\n",
    "    # Processat de tots els tuits del dataset\n",
    "    # Mostrant seguiment per pantalla, dia a dia.\n",
    "    time_start0 = time.time()\n",
    "    time_start = time.time()\n",
    "    m=1 ; dia=0\n",
    "    for row in dataset.iterrows():\n",
    "        text = row[1]['text_y']\n",
    "        # Eliminem parts del text que no ens aporten significat\n",
    "        # Elimina stopwords, hashtags i referències a usuaris.\n",
    "        # Normalitzem el text, per reduir la diversitat de paraules al tuit.\n",
    "        Netejat = NetejaNorm(text, idioma)\n",
    "        # Obtenim el text netejat (abans del caràcter @) \n",
    "        # i el normalitzat (després del caràcter @)\n",
    "        Netejat=Netejat.split('@')\n",
    "        textClean.append(Netejat[0])\n",
    "        # Calcula els emojis continguts al text.\n",
    "        emojis.append(cerca_emojis(text))\n",
    "        textNorm.append(Netejat[1])\n",
    "        # Utilitzem la llibreria 'textblob' per calcular\n",
    "        # Polaritat i Subjectivitat.\n",
    "        polarity.append(TextBlob(row[1]['text_y']).sentiment.polarity)\n",
    "        subjectivity.append(TextBlob(row[1]['text_y']).sentiment.subjectivity)\n",
    "        if m!=1:\n",
    "            if row[1]['dia']!=dia:\n",
    "                temps=(time.time()-time_start)/60\n",
    "                print(\"Data: {}/{}/{}\".format(dia,mes,yy),\"\\n#Tuits processats:\", \\\n",
    "                      m-1,\"\\n Durada: \", \\\n",
    "                      int(temps) if temps>0 else 0,\"minut/s \", int((temps-int(temps))*60), \\\n",
    "                      \"segons.\")\n",
    "                m=1\n",
    "                time_start = time.time()\n",
    "        dia=row[1]['dia'] ; mes=row[1]['mes'] ; yy=row[1]['yy']\n",
    "        m=m+1\n",
    "    # Temps parcial diari.\n",
    "    temps=(time.time()-time_start)/60\n",
    "    print(\"Data: {}/{}/{}\".format(dia,mes,yy),\"\\n#Tuits processats:\",m,\"\\n Durada: \", \\\n",
    "          int(temps) if temps>0 else 0,\"minut/s \", \\\n",
    "          int((temps-int(temps))*60),\"segons.\")\n",
    "    # Temps total\n",
    "    temps=(time.time()-time_start0)/60\n",
    "    print(\"#Total de tuits processats:\",dataset.shape[0],\"\\n Durada: \",int(temps) if temps>0 else 0, \\\n",
    "          \"minut/s \", \\\n",
    "          int((temps-int(temps))*60),\"segons.\")\n",
    "    \n",
    "    df_net_norm=pd.DataFrame({'text_net':textClean,'text_Norm':textNorm,'emojis':emojis, \\\n",
    "                              'polarity':polarity,'subjectivity':subjectivity})\n",
    "    \n",
    "    return df_net_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: 13/2/2020 \n",
      "#Tuits processats: 1223 \n",
      " Durada:  0 minut/s  8 segons.\n",
      "Data: 14/2/2020 \n",
      "#Tuits processats: 284 \n",
      " Durada:  0 minut/s  1 segons.\n",
      "Data: 15/2/2020 \n",
      "#Tuits processats: 245 \n",
      " Durada:  0 minut/s  1 segons.\n",
      "Data: 16/2/2020 \n",
      "#Tuits processats: 163 \n",
      " Durada:  0 minut/s  1 segons.\n",
      "Data: 17/2/2020 \n",
      "#Tuits processats: 220 \n",
      " Durada:  0 minut/s  1 segons.\n",
      "Data: 18/2/2020 \n",
      "#Tuits processats: 71 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Data: 19/2/2020 \n",
      "#Tuits processats: 833 \n",
      " Durada:  0 minut/s  6 segons.\n",
      "Data: 20/2/2020 \n",
      "#Tuits processats: 987 \n",
      " Durada:  0 minut/s  7 segons.\n",
      "Data: 21/2/2020 \n",
      "#Tuits processats: 1471 \n",
      " Durada:  0 minut/s  11 segons.\n",
      "Data: 22/2/2020 \n",
      "#Tuits processats: 945 \n",
      " Durada:  0 minut/s  7 segons.\n",
      "Data: 23/2/2020 \n",
      "#Tuits processats: 794 \n",
      " Durada:  0 minut/s  6 segons.\n",
      "Data: 24/2/2020 \n",
      "#Tuits processats: 2221 \n",
      " Durada:  0 minut/s  17 segons.\n",
      "Data: 25/2/2020 \n",
      "#Tuits processats: 2602 \n",
      " Durada:  0 minut/s  19 segons.\n",
      "Data: 26/2/2020 \n",
      "#Tuits processats: 2815 \n",
      " Durada:  0 minut/s  21 segons.\n",
      "Data: 27/2/2020 \n",
      "#Tuits processats: 3425 \n",
      " Durada:  0 minut/s  26 segons.\n",
      "Data: 28/2/2020 \n",
      "#Tuits processats: 11355 \n",
      " Durada:  1 minut/s  24 segons.\n",
      "Data: 29/2/2020 \n",
      "#Tuits processats: 43749 \n",
      " Durada:  5 minut/s  33 segons.\n",
      "Data: 1/3/2020 \n",
      "#Tuits processats: 1 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Data: 29/2/2020 \n",
      "#Tuits processats: 1 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Data: 1/3/2020 \n",
      "#Tuits processats: 12262 \n",
      " Durada:  1 minut/s  32 segons.\n",
      "Data: 2/3/2020 \n",
      "#Tuits processats: 3764 \n",
      " Durada:  0 minut/s  28 segons.\n",
      "Data: 3/3/2020 \n",
      "#Tuits processats: 1632 \n",
      " Durada:  0 minut/s  12 segons.\n",
      "Data: 4/3/2020 \n",
      "#Tuits processats: 1262 \n",
      " Durada:  0 minut/s  8 segons.\n",
      "Data: 5/3/2020 \n",
      "#Tuits processats: 1506 \n",
      " Durada:  0 minut/s  10 segons.\n",
      "Data: 6/3/2020 \n",
      "#Tuits processats: 773 \n",
      " Durada:  0 minut/s  5 segons.\n",
      "Data: 7/3/2020 \n",
      "#Tuits processats: 456 \n",
      " Durada:  0 minut/s  3 segons.\n",
      "Data: 8/3/2020 \n",
      "#Tuits processats: 255 \n",
      " Durada:  0 minut/s  1 segons.\n",
      "Data: 9/3/2020 \n",
      "#Tuits processats: 292 \n",
      " Durada:  0 minut/s  2 segons.\n",
      "Data: 10/3/2020 \n",
      "#Tuits processats: 253 \n",
      " Durada:  0 minut/s  1 segons.\n",
      "Data: 11/3/2020 \n",
      "#Tuits processats: 229 \n",
      " Durada:  0 minut/s  1 segons.\n",
      "Data: 12/3/2020 \n",
      "#Tuits processats: 386 \n",
      " Durada:  0 minut/s  3 segons.\n",
      "Data: 13/3/2020 \n",
      "#Tuits processats: 319 \n",
      " Durada:  0 minut/s  2 segons.\n",
      "Data: 14/3/2020 \n",
      "#Tuits processats: 121 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Data: 15/3/2020 \n",
      "#Tuits processats: 140 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Data: 16/3/2020 \n",
      "#Tuits processats: 170 \n",
      " Durada:  0 minut/s  1 segons.\n",
      "Data: 17/3/2020 \n",
      "#Tuits processats: 82 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Data: 18/3/2020 \n",
      "#Tuits processats: 93 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Data: 19/3/2020 \n",
      "#Tuits processats: 78 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Data: 20/3/2020 \n",
      "#Tuits processats: 84 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Data: 21/3/2020 \n",
      "#Tuits processats: 79 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Data: 22/3/2020 \n",
      "#Tuits processats: 76 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Data: 23/3/2020 \n",
      "#Tuits processats: 2 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Data: 24/3/2020 \n",
      "#Tuits processats: 86 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Data: 25/3/2020 \n",
      "#Tuits processats: 144 \n",
      " Durada:  0 minut/s  1 segons.\n",
      "Data: 26/3/2020 \n",
      "#Tuits processats: 106 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Data: 27/3/2020 \n",
      "#Tuits processats: 99 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Data: 28/3/2020 \n",
      "#Tuits processats: 68 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Data: 29/3/2020 \n",
      "#Tuits processats: 94 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "Data: 30/3/2020 \n",
      "#Tuits processats: 118 \n",
      " Durada:  0 minut/s  0 segons.\n",
      "#Total de tuits processats: 98433 \n",
      " Durada:  12 minut/s  22 segons.\n"
     ]
    }
   ],
   "source": [
    "df_depurat = depuraTextTuit(dataset_final,'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actualizació del dataset final\n",
    "dataset_final.update(df_depurat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98433, 25)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exportació a disc en format Excel/csv/Pickle\n",
    "dataset_final.to_excel(\"DMMM_dataset_Final.xlsx\",index=0)\n",
    "dataset_final.to_csv(\"DMMM_dataset_Final.csv\", index = False, header=True)\n",
    "dataset_final.to_pickle(\"DMMM_dataset_Final.pkl\")\n",
    "dataset_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text_x</th>\n",
       "      <th>text_net</th>\n",
       "      <th>text_Norm</th>\n",
       "      <th>diaSem</th>\n",
       "      <th>dia</th>\n",
       "      <th>mes</th>\n",
       "      <th>yy</th>\n",
       "      <th>hora</th>\n",
       "      <th>...</th>\n",
       "      <th>user_idstr</th>\n",
       "      <th>user_friends_c</th>\n",
       "      <th>user_followers_c</th>\n",
       "      <th>user_listed_c</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>lang</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>emojis</th>\n",
       "      <th>text_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5e78e80f4e54db2148d19749</td>\n",
       "      <td>Thu Feb 13 09:58:19 +0000 2020</td>\n",
       "      <td>RT @FEDER_ONG: Las 29 obras ganadoras del conc...</td>\n",
       "      <td>winning works photographic contest form travel...</td>\n",
       "      <td>win work photograph contest form travel exhibi...</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>4054806561</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>[]</td>\n",
       "      <td>RT @FEDER_ONG: The 29 winning works of the pho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5e78e80f4e54db2148d1974a</td>\n",
       "      <td>Thu Feb 13 10:01:01 +0000 2020</td>\n",
       "      <td>📌 Esta tarde, a partir de las 21:00h, nuevo #C...</td>\n",
       "      <td>afternoon starting new centered</td>\n",
       "      <td>afternoon start new cent</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>64270216</td>\n",
       "      <td>853</td>\n",
       "      <td>2104</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "      <td>0.0681818</td>\n",
       "      <td>0.277273</td>\n",
       "      <td>[📌]</td>\n",
       "      <td>📌 This afternoon, starting at 21 :00h, new #Cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5e78e80f4e54db2148d1974b</td>\n",
       "      <td>Thu Feb 13 10:01:31 +0000 2020</td>\n",
       "      <td>RT @FEDER_ONG: Gracias a Emilio Butragueño, Da...</td>\n",
       "      <td>thanks emilio butragueño david de maria soleda...</td>\n",
       "      <td>thank emilio butragueño david de mar soledad g...</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>233031604</td>\n",
       "      <td>446</td>\n",
       "      <td>648</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>[]</td>\n",
       "      <td>RT @FEDER_ONG: Thanks to Emilio Butragueño, Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5e78e80f4e54db2148d1974c</td>\n",
       "      <td>Thu Feb 13 10:01:38 +0000 2020</td>\n",
       "      <td>Va a quedar tan bonito @nh487 ...\\n\\n#FebreroR...</td>\n",
       "      <td>going look beautiful</td>\n",
       "      <td>going look beauty</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>237839609</td>\n",
       "      <td>11718</td>\n",
       "      <td>19380</td>\n",
       "      <td>682</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>It's going to look so beautiful.\\n\\n#FebreroRa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5e78e80f4e54db2148d1974d</td>\n",
       "      <td>Thu Feb 13 10:02:02 +0000 2020</td>\n",
       "      <td>RT @ERdivulga: todo un honor tener a @PastorAl...</td>\n",
       "      <td>honor collaborating teaching subject physiopat...</td>\n",
       "      <td>hon collab teach subject physiopatholog</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>3002280588</td>\n",
       "      <td>236</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>[]</td>\n",
       "      <td>RT @ERdivulga: it is an honor to have @PastorA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id                      created_at  \\\n",
       "0  5e78e80f4e54db2148d19749  Thu Feb 13 09:58:19 +0000 2020   \n",
       "1  5e78e80f4e54db2148d1974a  Thu Feb 13 10:01:01 +0000 2020   \n",
       "2  5e78e80f4e54db2148d1974b  Thu Feb 13 10:01:31 +0000 2020   \n",
       "3  5e78e80f4e54db2148d1974c  Thu Feb 13 10:01:38 +0000 2020   \n",
       "4  5e78e80f4e54db2148d1974d  Thu Feb 13 10:02:02 +0000 2020   \n",
       "\n",
       "                                              text_x  \\\n",
       "0  RT @FEDER_ONG: Las 29 obras ganadoras del conc...   \n",
       "1  📌 Esta tarde, a partir de las 21:00h, nuevo #C...   \n",
       "2  RT @FEDER_ONG: Gracias a Emilio Butragueño, Da...   \n",
       "3  Va a quedar tan bonito @nh487 ...\\n\\n#FebreroR...   \n",
       "4  RT @ERdivulga: todo un honor tener a @PastorAl...   \n",
       "\n",
       "                                            text_net  \\\n",
       "0  winning works photographic contest form travel...   \n",
       "1                    afternoon starting new centered   \n",
       "2  thanks emilio butragueño david de maria soleda...   \n",
       "3                               going look beautiful   \n",
       "4  honor collaborating teaching subject physiopat...   \n",
       "\n",
       "                                           text_Norm    diaSem  dia  mes  \\\n",
       "0  win work photograph contest form travel exhibi...  Thursday   13    2   \n",
       "1                           afternoon start new cent  Thursday   13    2   \n",
       "2  thank emilio butragueño david de mar soledad g...  Thursday   13    2   \n",
       "3                                  going look beauty  Thursday   13    2   \n",
       "4            hon collab teach subject physiopatholog  Thursday   13    2   \n",
       "\n",
       "     yy  hora  ...  user_idstr  user_friends_c user_followers_c user_listed_c  \\\n",
       "0  2020     9  ...  4054806561               8                3             0   \n",
       "1  2020    10  ...    64270216             853             2104            31   \n",
       "2  2020    10  ...   233031604             446              648             9   \n",
       "3  2020    10  ...   237839609           11718            19380           682   \n",
       "4  2020    10  ...  3002280588             236              123             0   \n",
       "\n",
       "  retweet_count lang   polarity  subjectivity  emojis  \\\n",
       "0             0   es        0.5          0.75      []   \n",
       "1             0   es  0.0681818      0.277273     [📌]   \n",
       "2             0   es        0.2           0.2      []   \n",
       "3             0   es       0.85             1      []   \n",
       "4             0   es  -0.166667      0.333333      []   \n",
       "\n",
       "                                              text_y  \n",
       "0  RT @FEDER_ONG: The 29 winning works of the pho...  \n",
       "1  📌 This afternoon, starting at 21 :00h, new #Cr...  \n",
       "2  RT @FEDER_ONG: Thanks to Emilio Butragueño, Da...  \n",
       "3  It's going to look so beautiful.\\n\\n#FebreroRa...  \n",
       "4  RT @ERdivulga: it is an honor to have @PastorA...  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
