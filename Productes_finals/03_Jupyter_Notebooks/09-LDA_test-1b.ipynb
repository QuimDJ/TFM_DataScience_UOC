{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def temes(d,prob=False):\n",
    "    clusters=[]\n",
    "    l=[]\n",
    "    for i in range(0,d.shape[0]): \n",
    "        if prob:\n",
    "                print(\"Cluster {}: {}\".format(i,d.text[i]))\n",
    "                clusters.append(i)\n",
    "                l.append(d.text[i])\n",
    "        else:\n",
    "            clusters.append(i)\n",
    "            paraules=[k.replace('\"','') for k in re.findall(r'\\\".*?\\\"',d.text[i])]\n",
    "            l.append(paraules)\n",
    "            print(\"Cluster {}: {}\".format(i,paraules))\n",
    "                \n",
    "    r=pd.DataFrame({'clusters':clusters,'paraules':l})       \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# llegim dataset\n",
    "tuits = pd.read_excel(\"c:/users/qdeda/TFM_Code/DMMM_dataset_Final.xlsx\")\n",
    "\n",
    "# Transformacions:\n",
    "t=tuits.drop(tuits[tuits.text_y.str.startswith('RT')].index)\n",
    "# Juntem tuits d'un mateix autor.\n",
    "t1 = t[['text_net','text_Norm','user_idstr']].groupby(['user_idstr']).agg(' '.join)\n",
    "# Mitjana de polaritats i subjectivitats de tots els tuits\n",
    "pol_subj = t[['user_idstr','polarity','subjectivity']].groupby(['user_idstr']).agg('mean')\n",
    "\n",
    "# Eliminen paraules repetides\n",
    "for row in t1.iterrows():\n",
    "    tmp1=' '.join(set(row[1]['text_net'].split()))\n",
    "    row[1]['text_net']=tmp1\n",
    "    tmp2=' ' .join(set(row[1]['text_Norm'].split()))\n",
    "    row[1]['text_Norm']=tmp2\n",
    "# Reconstruim el dataset indexat per autor, perdem les identitats dels tuits\n",
    "# tenim usuaris / paraules al construir el vector tfidf.\n",
    "tuits = pd.merge(t1, pol_subj, left_on=t1.index, right_on=pol_subj.index, how='inner')\n",
    "tuits.columns=['autor','text','text_norm','polaritat','subjectivitat']\n",
    "\n",
    "# observem que les paraules obvies de hashtags queden per eliminar\n",
    "excloure=['rare','day','today','february','disease','diseases','world',\n",
    "         'dr','feb','st','th','thank','to','us','we','years']\n",
    "for p in excloure:\n",
    "    tuits.text=tuits.text.str.replace(p,\"\")\n",
    "tuits.text\n",
    "\n",
    "tuits=tuits.drop(tuits[tuits.text.str.len()==0].index)\n",
    "tuits.reset_index(drop=True, inplace=True)\n",
    "t=[]\n",
    "t1=[]\n",
    "t2=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12368, 219)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(min_df=0.01)\n",
    "matriu_tfidf = tfidf_vect.fit_transform(tuits.text)\n",
    "matriu_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(tfidf_vect.vocabulary_.items(), key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean=tuits.text\n",
    "doc_clean=list(doc_clean.str.split())\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "doc_term_matrix\n",
    "\n",
    "Lda = gensim.models.ldamodel.LdaModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temàtiques per LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# executem LDA per 3 temes\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "\n",
    "print(ldamodel.print_topics())\n",
    "d3=pd.DataFrame(ldamodel.print_topics(),columns=[\"topic\",\"text\"])\n",
    "temes(d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# executem LDA per 6 temes\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=6, id2word = dictionary, passes=50)\n",
    "\n",
    "print(ldamodel.print_topics())\n",
    "d6=pd.DataFrame(ldamodel.print_topics(),columns=[\"topic\",\"text\"])\n",
    "temes(d6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# executem LDA per 8 temes\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=8, id2word = dictionary, passes=50)\n",
    "\n",
    "print(ldamodel.print_topics())\n",
    "d8=pd.DataFrame(ldamodel.print_topics(),columns=[\"topic\",\"text\"])\n",
    "temes(d8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# executem LDA per 10 temes\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=10, id2word = dictionary, passes=50)\n",
    "\n",
    "print(ldamodel.print_topics())\n",
    "d10=pd.DataFrame(ldamodel.print_topics(),columns=[\"topic\",\"text\"])\n",
    "temes(d10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# executem LDA per 13 temes\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=13, id2word = dictionary, passes=50)\n",
    "\n",
    "print(ldamodel.print_topics())\n",
    "d13=pd.DataFrame(ldamodel.print_topics(),columns=[\"topic\",\"text\"])\n",
    "temes(d13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.017*\"wish\" + 0.017*\"create\" + 0.016*\"right\" + 0.015*\"national\" + 0.012*\"ings\" + 0.012*\"matter\" + 0.012*\"received\" + 0.011*\"man\" + 0.010*\"oh\" + 0.010*\"kids\"'), (1, '0.051*\"happy\" + 0.026*\"opportunity\" + 0.024*\"full\" + 0.019*\"might\" + 0.018*\"ill\" + 0.018*\"research\" + 0.018*\"care\" + 0.018*\"life\" + 0.017*\"geer\" + 0.017*\"opportunities\"'), (2, '0.044*\"ank\" + 0.021*\"great\" + 0.020*\"much\" + 0.020*\"ll\" + 0.017*\"video\" + 0.016*\"amazing\" + 0.014*\"good\" + 0.013*\"work\" + 0.012*\"via\" + 0.011*\"done\"'), (3, '0.036*\"s\" + 0.014*\"morrow\" + 0.012*\"celebrated\" + 0.012*\"international\" + 0.010*\"la\" + 0.009*\"patient\" + 0.009*\"research\" + 0.008*\"year\" + 0.008*\"event\" + 0.008*\"satur\"'), (4, '0.029*\"care\" + 0.026*\"heal\" + 0.022*\"campaign\" + 0.018*\"access\" + 0.017*\"social\" + 0.016*\"diagnosis\" + 0.015*\"s\" + 0.015*\"treatment\" + 0.015*\"event\" + 0.013*\"official\"'), (5, '0.036*\"love\" + 0.024*\"little\" + 0.018*\"sick\" + 0.017*\"want\" + 0.015*\"bit\" + 0.014*\"op\" + 0.012*\"de\" + 0.010*\"happen\" + 0.010*\"put\" + 0.009*\"send\"'), (6, '0.026*\"know\" + 0.023*\"one\" + 0.016*\"s\" + 0.015*\"people\" + 0.014*\"like\" + 0.012*\"also\" + 0.012*\"life\" + 0.011*\"many\" + 0.011*\"even\" + 0.010*\"need\"'), (7, '0.024*\"synome\" + 0.018*\"disorder\" + 0.017*\"diagnosed\" + 0.016*\"cancer\" + 0.015*\"called\" + 0.013*\"condition\" + 0.013*\"genetic\" + 0.012*\"type\" + 0.011*\"caes\" + 0.010*\"like\"'), (8, '0.048*\"anks\" + 0.046*\"late\" + 0.035*\"daily\" + 0.024*\"coronavir\" + 0.019*\"news\" + 0.010*\"unfortunately\" + 0.010*\"intereing\" + 0.009*\"oand\" + 0.008*\"defined\" + 0.008*\"hours\"'), (9, '0.013*\"new\" + 0.012*\"time\" + 0.012*\"find\" + 0.012*\"looking\" + 0.011*\"s\" + 0.010*\"better\" + 0.010*\"link\" + 0.010*\"research\" + 0.010*\"synome\" + 0.009*\"spain\"'), (10, '0.045*\"read\" + 0.030*\"ory\" + 0.027*\"sharing\" + 0.020*\"beautiful\" + 0.020*\"become\" + 0.019*\"families\" + 0.019*\"experience\" + 0.018*\"supporting\" + 0.017*\"hope\" + 0.015*\"ar\"'), (11, '0.028*\"support\" + 0.022*\"help\" + 0.020*\"awareness\" + 0.016*\"ank\" + 0.012*\"please\" + 0.011*\"share\" + 0.010*\"get\" + 0.010*\"work\" + 0.009*\"raise\" + 0.009*\"patients\"'), (12, '0.052*\"s\" + 0.047*\"people\" + 0.031*\"awareness\" + 0.027*\"million\" + 0.022*\"living\" + 0.019*\"patients\" + 0.018*\"raise\" + 0.013*\"support\" + 0.012*\"wide\" + 0.012*\"many\"'), (13, '0.015*\"child\" + 0.011*\"school\" + 0.010*\"lack\" + 0.010*\"europe\" + 0.010*\"old\" + 0.010*\"son\" + 0.008*\"life\" + 0.008*\"mean\" + 0.008*\"eye\" + 0.008*\"born\"'), (14, '0.018*\"remember\" + 0.017*\"forget\" + 0.013*\"big\" + 0.012*\"make\" + 0.012*\"s\" + 0.012*\"good\" + 0.010*\"ay\" + 0.010*\"re\" + 0.010*\"play\" + 0.009*\"march\"')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017*\"wish\" + 0.017*\"create\" + 0.016*\"right\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.051*\"happy\" + 0.026*\"opportunity\" + 0.024*\"f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.044*\"ank\" + 0.021*\"great\" + 0.020*\"much\" + 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.036*\"s\" + 0.014*\"morrow\" + 0.012*\"celebrated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.029*\"care\" + 0.026*\"heal\" + 0.022*\"campaign\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.036*\"love\" + 0.024*\"little\" + 0.018*\"sick\" +...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.026*\"know\" + 0.023*\"one\" + 0.016*\"s\" + 0.015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.024*\"synome\" + 0.018*\"disorder\" + 0.017*\"dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.048*\"anks\" + 0.046*\"late\" + 0.035*\"daily\" + ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.013*\"new\" + 0.012*\"time\" + 0.012*\"find\" + 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.045*\"read\" + 0.030*\"ory\" + 0.027*\"sharing\" +...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0.028*\"support\" + 0.022*\"help\" + 0.020*\"awaren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0.052*\"s\" + 0.047*\"people\" + 0.031*\"awareness\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0.015*\"child\" + 0.011*\"school\" + 0.010*\"lack\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0.018*\"remember\" + 0.017*\"forget\" + 0.013*\"big...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic                                               text\n",
       "0       0  0.017*\"wish\" + 0.017*\"create\" + 0.016*\"right\" ...\n",
       "1       1  0.051*\"happy\" + 0.026*\"opportunity\" + 0.024*\"f...\n",
       "2       2  0.044*\"ank\" + 0.021*\"great\" + 0.020*\"much\" + 0...\n",
       "3       3  0.036*\"s\" + 0.014*\"morrow\" + 0.012*\"celebrated...\n",
       "4       4  0.029*\"care\" + 0.026*\"heal\" + 0.022*\"campaign\"...\n",
       "5       5  0.036*\"love\" + 0.024*\"little\" + 0.018*\"sick\" +...\n",
       "6       6  0.026*\"know\" + 0.023*\"one\" + 0.016*\"s\" + 0.015...\n",
       "7       7  0.024*\"synome\" + 0.018*\"disorder\" + 0.017*\"dia...\n",
       "8       8  0.048*\"anks\" + 0.046*\"late\" + 0.035*\"daily\" + ...\n",
       "9       9  0.013*\"new\" + 0.012*\"time\" + 0.012*\"find\" + 0....\n",
       "10     10  0.045*\"read\" + 0.030*\"ory\" + 0.027*\"sharing\" +...\n",
       "11     11  0.028*\"support\" + 0.022*\"help\" + 0.020*\"awaren...\n",
       "12     12  0.052*\"s\" + 0.047*\"people\" + 0.031*\"awareness\"...\n",
       "13     13  0.015*\"child\" + 0.011*\"school\" + 0.010*\"lack\" ...\n",
       "14     14  0.018*\"remember\" + 0.017*\"forget\" + 0.013*\"big..."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# executem LDA per 15 temes\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=15, id2word =dictionary, passes=50)\n",
    "\n",
    "print(ldamodel.print_topics())\n",
    "d15=pd.DataFrame(ldamodel.print_topics(),columns=[\"topic\",\"text\"])\n",
    "d15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: ['wish', 'create', 'right', 'national', 'ings', 'matter', 'received', 'man', 'oh', 'kids']\n",
      "Cluster 1: ['happy', 'opportunity', 'full', 'might', 'ill', 'research', 'care', 'life', 'geer', 'opportunities']\n",
      "Cluster 2: ['ank', 'great', 'much', 'll', 'video', 'amazing', 'good', 'work', 'via', 'done']\n",
      "Cluster 3: ['s', 'morrow', 'celebrated', 'international', 'la', 'patient', 'research', 'year', 'event', 'satur']\n",
      "Cluster 4: ['care', 'heal', 'campaign', 'access', 'social', 'diagnosis', 's', 'treatment', 'event', 'official']\n",
      "Cluster 5: ['love', 'little', 'sick', 'want', 'bit', 'op', 'de', 'happen', 'put', 'send']\n",
      "Cluster 6: ['know', 'one', 's', 'people', 'like', 'also', 'life', 'many', 'even', 'need']\n",
      "Cluster 7: ['synome', 'disorder', 'diagnosed', 'cancer', 'called', 'condition', 'genetic', 'type', 'caes', 'like']\n",
      "Cluster 8: ['anks', 'late', 'daily', 'coronavir', 'news', 'unfortunately', 'intereing', 'oand', 'defined', 'hours']\n",
      "Cluster 9: ['new', 'time', 'find', 'looking', 's', 'better', 'link', 'research', 'synome', 'spain']\n",
      "Cluster 10: ['read', 'ory', 'sharing', 'beautiful', 'become', 'families', 'experience', 'supporting', 'hope', 'ar']\n",
      "Cluster 11: ['support', 'help', 'awareness', 'ank', 'please', 'share', 'get', 'work', 'raise', 'patients']\n",
      "Cluster 12: ['s', 'people', 'awareness', 'million', 'living', 'patients', 'raise', 'support', 'wide', 'many']\n",
      "Cluster 13: ['child', 'school', 'lack', 'europe', 'old', 'son', 'life', 'mean', 'eye', 'born']\n",
      "Cluster 14: ['remember', 'forget', 'big', 'make', 's', 'good', 'ay', 're', 'play', 'march']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clusters</th>\n",
       "      <th>paraules</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[wish, create, right, national, ings, matter, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[happy, opportunity, full, might, ill, researc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[ank, great, much, ll, video, amazing, good, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[s, morrow, celebrated, international, la, pat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[care, heal, campaign, access, social, diagnos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[love, little, sick, want, bit, op, de, happen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>[know, one, s, people, like, also, life, many,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>[synome, disorder, diagnosed, cancer, called, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[anks, late, daily, coronavir, news, unfortuna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[new, time, find, looking, s, better, link, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[read, ory, sharing, beautiful, become, famili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>[support, help, awareness, ank, please, share,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>[s, people, awareness, million, living, patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[child, school, lack, europe, old, son, life, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>[remember, forget, big, make, s, good, ay, re,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    clusters                                           paraules\n",
       "0          0  [wish, create, right, national, ings, matter, ...\n",
       "1          1  [happy, opportunity, full, might, ill, researc...\n",
       "2          2  [ank, great, much, ll, video, amazing, good, w...\n",
       "3          3  [s, morrow, celebrated, international, la, pat...\n",
       "4          4  [care, heal, campaign, access, social, diagnos...\n",
       "5          5  [love, little, sick, want, bit, op, de, happen...\n",
       "6          6  [know, one, s, people, like, also, life, many,...\n",
       "7          7  [synome, disorder, diagnosed, cancer, called, ...\n",
       "8          8  [anks, late, daily, coronavir, news, unfortuna...\n",
       "9          9  [new, time, find, looking, s, better, link, re...\n",
       "10        10  [read, ory, sharing, beautiful, become, famili...\n",
       "11        11  [support, help, awareness, ank, please, share,...\n",
       "12        12  [s, people, awareness, million, living, patien...\n",
       "13        13  [child, school, lack, europe, old, son, life, ...\n",
       "14        14  [remember, forget, big, make, s, good, ay, re,..."
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temes(d15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuits.text=tuits['text'].replace(\"rong\",\"wrong\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>autor</th>\n",
       "      <th>text</th>\n",
       "      <th>text_norm</th>\n",
       "      <th>polaritat</th>\n",
       "      <th>subjectivitat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>284633</td>\n",
       "      <td>None</td>\n",
       "      <td>th febru understand behind shitty ide sery lit...</td>\n",
       "      <td>-0.306944</td>\n",
       "      <td>0.622222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>286543</td>\n",
       "      <td>None</td>\n",
       "      <td>nient</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>610873</td>\n",
       "      <td>None</td>\n",
       "      <td>rol suit build play lik many reorg ear thing c...</td>\n",
       "      <td>0.178329</td>\n",
       "      <td>0.623134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>627213</td>\n",
       "      <td>None</td>\n",
       "      <td>common cel danlo nobody ehl immunodeficy neuro...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>681573</td>\n",
       "      <td>None</td>\n",
       "      <td>count idiocr diseas</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    autor  text                                          text_norm  polaritat  \\\n",
       "0  284633  None  th febru understand behind shitty ide sery lit...  -0.306944   \n",
       "1  286543  None                                              nient   0.000000   \n",
       "2  610873  None  rol suit build play lik many reorg ear thing c...   0.178329   \n",
       "3  627213  None  common cel danlo nobody ehl immunodeficy neuro...   0.250000   \n",
       "4  681573  None                                count idiocr diseas   0.000000   \n",
       "\n",
       "   subjectivitat  \n",
       "0       0.622222  \n",
       "1       0.000000  \n",
       "2       0.623134  \n",
       "3       0.650000  \n",
       "4       0.000000  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot index with vector containing NA / NaN values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-af33e81c5d00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtuits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtuits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rong'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\conda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1105\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1107\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1108\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda3\\lib\\site-packages\\pandas\\core\\common.py\u001b[0m in \u001b[0;36mis_bool_indexer\u001b[1;34m(key)\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mna_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot index with vector containing NA / NaN values"
     ]
    }
   ],
   "source": [
    "tuits.text[tuits.text.str.contains('rong')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>autor</th>\n",
       "      <th>text</th>\n",
       "      <th>text_norm</th>\n",
       "      <th>polaritat</th>\n",
       "      <th>subjectivitat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>284633</td>\n",
       "      <td>little  idea visibilising underand behind shi...</td>\n",
       "      <td>th febru understand behind shitty ide sery lit...</td>\n",
       "      <td>-0.306944</td>\n",
       "      <td>0.622222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>286543</td>\n",
       "      <td>niente</td>\n",
       "      <td>nient</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>610873</td>\n",
       "      <td>taking cutting scientis genetic build fibrosis...</td>\n",
       "      <td>rol suit build play lik many reorg ear thing c...</td>\n",
       "      <td>0.178329</td>\n",
       "      <td>0.623134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>627213</td>\n",
       "      <td>disorder common neurological nobody probably f...</td>\n",
       "      <td>common cel danlo nobody ehl immunodeficy neuro...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>681573</td>\n",
       "      <td>idiocracy count</td>\n",
       "      <td>count idiocr diseas</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12363</td>\n",
       "      <td>1235659467330641920</td>\n",
       "      <td>may guide give god good heal inshallah continue</td>\n",
       "      <td>may god good guid heal inshallah continu giv</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12364</td>\n",
       "      <td>1238808580419129088</td>\n",
       "      <td>ank much intereing video</td>\n",
       "      <td>thank much video interest</td>\n",
       "      <td>0.536250</td>\n",
       "      <td>0.455000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12365</td>\n",
       "      <td>1239512259027755008</td>\n",
       "      <td>infect addition oands sanidad</td>\n",
       "      <td>addit thousand infect sanidad</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12366</td>\n",
       "      <td>1243223320243441920</td>\n",
       "      <td>bro love lil aww</td>\n",
       "      <td>bro lov aww lil</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12367</td>\n",
       "      <td>1244048152937804032</td>\n",
       "      <td>vires ops op spread help vir fa runs</td>\n",
       "      <td>vir run us stop spread help virus fa</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12368 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     autor                                               text  \\\n",
       "0                   284633   little  idea visibilising underand behind shi...   \n",
       "1                   286543                                             niente   \n",
       "2                   610873  taking cutting scientis genetic build fibrosis...   \n",
       "3                   627213  disorder common neurological nobody probably f...   \n",
       "4                   681573                                    idiocracy count   \n",
       "...                    ...                                                ...   \n",
       "12363  1235659467330641920    may guide give god good heal inshallah continue   \n",
       "12364  1238808580419129088                           ank much intereing video   \n",
       "12365  1239512259027755008                      infect addition oands sanidad   \n",
       "12366  1243223320243441920                                   bro love lil aww   \n",
       "12367  1244048152937804032               vires ops op spread help vir fa runs   \n",
       "\n",
       "                                               text_norm  polaritat  \\\n",
       "0      th febru understand behind shitty ide sery lit...  -0.306944   \n",
       "1                                                  nient   0.000000   \n",
       "2      rol suit build play lik many reorg ear thing c...   0.178329   \n",
       "3      common cel danlo nobody ehl immunodeficy neuro...   0.250000   \n",
       "4                                    count idiocr diseas   0.000000   \n",
       "...                                                  ...        ...   \n",
       "12363       may god good guid heal inshallah continu giv   0.700000   \n",
       "12364                          thank much video interest   0.536250   \n",
       "12365                      addit thousand infect sanidad   0.000000   \n",
       "12366                                    bro lov aww lil   0.400000   \n",
       "12367               vir run us stop spread help virus fa   0.000000   \n",
       "\n",
       "       subjectivitat  \n",
       "0           0.622222  \n",
       "1           0.000000  \n",
       "2           0.623134  \n",
       "3           0.650000  \n",
       "4           0.000000  \n",
       "...              ...  \n",
       "12363       0.600000  \n",
       "12364       0.455000  \n",
       "12365       0.000000  \n",
       "12366       0.750000  \n",
       "12367       0.000000  \n",
       "\n",
       "[12368 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
